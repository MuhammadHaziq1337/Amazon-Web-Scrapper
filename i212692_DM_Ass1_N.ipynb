{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n\tGetHandleVerifier [0x00007FF6B80F3E42+31618]\n\t(No symbol) [0x00007FF6B806B0A9]\n\t(No symbol) [0x00007FF6B7F2888A]\n\t(No symbol) [0x00007FF6B7F78524]\n\t(No symbol) [0x00007FF6B7F7862C]\n\t(No symbol) [0x00007FF6B7FBF787]\n\t(No symbol) [0x00007FF6B7F9D14F]\n\t(No symbol) [0x00007FF6B7FBCA80]\n\t(No symbol) [0x00007FF6B7F9CEB3]\n\t(No symbol) [0x00007FF6B7F6A46B]\n\t(No symbol) [0x00007FF6B7F6B001]\n\tGetHandleVerifier [0x00007FF6B83FA01D+3202397]\n\tGetHandleVerifier [0x00007FF6B8446A3D+3516285]\n\tGetHandleVerifier [0x00007FF6B843C4B0+3473904]\n\tGetHandleVerifier [0x00007FF6B81A5D46+760454]\n\t(No symbol) [0x00007FF6B8076B4F]\n\t(No symbol) [0x00007FF6B8071CE4]\n\t(No symbol) [0x00007FF6B8071E72]\n\t(No symbol) [0x00007FF6B806121F]\n\tBaseThreadInitThunk [0x00007FFDC8E77034+20]\n\tRtlUserThreadStart [0x00007FFDCA822651+33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 153\u001b[0m\n\u001b[0;32m    150\u001b[0m write_to_file(product_info)  \u001b[38;5;66;03m# Write initial product info to file\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# Extract colors, images, and reviews\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m product_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mextract_colors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproduct_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m product_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReviews\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_reviews(driver, product_info)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Print the scraped data\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [6], line 35\u001b[0m, in \u001b[0;36mextract_colors\u001b[1;34m(driver, product_info)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_colors\u001b[39m(driver, product_info):\n\u001b[0;32m     33\u001b[0m     color_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement_to_be_clickable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m//li[contains(@id, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolor_name_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     options \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//li[contains(@id, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor_name_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m options:\n",
      "File \u001b[1;32mf:\\Pythan\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n\tGetHandleVerifier [0x00007FF6B80F3E42+31618]\n\t(No symbol) [0x00007FF6B806B0A9]\n\t(No symbol) [0x00007FF6B7F2888A]\n\t(No symbol) [0x00007FF6B7F78524]\n\t(No symbol) [0x00007FF6B7F7862C]\n\t(No symbol) [0x00007FF6B7FBF787]\n\t(No symbol) [0x00007FF6B7F9D14F]\n\t(No symbol) [0x00007FF6B7FBCA80]\n\t(No symbol) [0x00007FF6B7F9CEB3]\n\t(No symbol) [0x00007FF6B7F6A46B]\n\t(No symbol) [0x00007FF6B7F6B001]\n\tGetHandleVerifier [0x00007FF6B83FA01D+3202397]\n\tGetHandleVerifier [0x00007FF6B8446A3D+3516285]\n\tGetHandleVerifier [0x00007FF6B843C4B0+3473904]\n\tGetHandleVerifier [0x00007FF6B81A5D46+760454]\n\t(No symbol) [0x00007FF6B8076B4F]\n\t(No symbol) [0x00007FF6B8071CE4]\n\t(No symbol) [0x00007FF6B8071E72]\n\t(No symbol) [0x00007FF6B806121F]\n\tBaseThreadInitThunk [0x00007FFDC8E77034+20]\n\tRtlUserThreadStart [0x00007FFDCA822651+33]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def write_to_file(data, filename='products_combined.json'):\n",
    "    with open(filename, 'a', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "        file.write('\\n')\n",
    "\n",
    "def extract_image(main_driver):\n",
    "    image_dict = {}\n",
    "    try:\n",
    "        landing_image = main_driver.find_element(By.ID, \"landingImage\")\n",
    "        landing_image = landing_image.get_attribute('src')\n",
    "        image_dict['Landing_Image'] = landing_image\n",
    "    except:\n",
    "        image_dict['Landing_Image'] = None\n",
    "    images = main_driver.find_elements(By.XPATH, '//div[@id=\"altImages\"]//li[contains(@class, \"item\")]//img')\n",
    "    i_list = []\n",
    "    for image in images:\n",
    "        image = image.get_attribute('src')\n",
    "        i_list.append(image)\n",
    "    image_dict['Other_Images'] = i_list\n",
    "    return image_dict\n",
    "\n",
    "def extract_colors(driver, product_info):\n",
    "    color_list = []\n",
    "  \n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//li[contains(@id, 'color_name_')]\"))\n",
    "    )\n",
    "    options = driver.find_elements(By.XPATH, \"//li[contains(@id, 'color_name_')]\")\n",
    "\n",
    "    for option in options:\n",
    "        color_span = option.find_element(By.XPATH, \".//span[@class='a-list-item']\")\n",
    "        color_span.click()\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "\n",
    "        color_dict = {}\n",
    "        images = extract_image(driver)\n",
    "        try:\n",
    "            color_name = driver.find_element(By.ID, 'variation_color_name').find_element(By.CSS_SELECTOR, 'span.selection').text\n",
    "            color_dict['Color'] = color_name\n",
    "            color_dict['Images'] = images\n",
    "            color_list.append(color_dict)\n",
    "            \n",
    "            # Write color information to file\n",
    "            product_info['Colors'] = color_list\n",
    "            write_to_file(product_info)\n",
    "        except Exception as e:\n",
    "            print(\"Error extracting color:\", e)\n",
    "            continue\n",
    "    return color_list\n",
    "\n",
    "def extract_reviews(main_driver, product_info):\n",
    "    reviews=[]\n",
    "    try:\n",
    "        print(\"Fetching Reviews\")\n",
    "        review_path=main_driver.find_element(By.XPATH,\"//a[normalize-space()='See more reviews']\")\n",
    "        review_link=review_path.get_attribute('href')\n",
    "        main_driver.execute_script(\"window.open(arguments[0], '_blank');\", review_link)\n",
    "        main_driver.switch_to.window(main_driver.window_handles[-1])\n",
    "        time.sleep(random.uniform(3, 5)) \n",
    "        for i in range(0,10):\n",
    "            extract_stars=main_driver.find_elements(By.XPATH,\"//i[@data-hook='review-star-rating']\")\n",
    "            extract_content=main_driver.find_elements(By.XPATH,\"//span[@data-hook='review-body']\")\n",
    "            for star_element, content_element in zip(extract_stars, extract_content):\n",
    "                if star_element and content_element is not None:\n",
    "                    star_rating = star_element.get_attribute(\"class\")\n",
    "                    review_content = content_element.text\n",
    "                    review={}\n",
    "                    review['stars'] = star_rating\n",
    "                    review['content'] = review_content\n",
    "                    reviews.append(review)\n",
    "\n",
    "                    # Write review information to file\n",
    "                    product_info['Reviews'] = reviews\n",
    "                    write_to_file(product_info)\n",
    "            try:\n",
    "                next_page=main_driver.find_element(By.XPATH,\"//a[contains(text(),'Next page')]\")\n",
    "                next_page.click()\n",
    "                time.sleep(random.uniform(3, 5)) \n",
    "            except:\n",
    "                main_driver.close()\n",
    "                main_driver.switch_to.window(main_driver.window_handles[-1])\n",
    "                time.sleep(random.uniform(3, 5)) \n",
    "                return reviews\n",
    "    except:\n",
    "        time.sleep(random.uniform(3, 5)) \n",
    "        return None\n",
    "    time.sleep(2)\n",
    "    main_driver.close()\n",
    "    main_driver.switch_to.window(main_driver.window_handles[-1])\n",
    "    time.sleep(random.uniform(3, 5)) \n",
    "    return reviews\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "chrome_options = Options()\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.89 Safari/537.36\"\n",
    "chrome_options.add_argument(f\"user-agent={USER_AGENT}\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "driver.get(\"https://www.amazon.com/s?i=specialty-aps&bbn=16225019011&rh=n%3A7141123011%2Cn%3A16225019011%2Cn%3A679255011&ref=nav_em__nav_desktop_sa_intl_shoes_0_2_13_3\")\n",
    "time.sleep(5)  \n",
    "\n",
    "# Extract product URLs\n",
    "product_links = driver.find_elements(By.XPATH, '//div[contains(@class, \"s-result-item\")]//h2/a')\n",
    "product_urls = [link.get_attribute('href') for link in product_links]\n",
    "\n",
    "# Visit each product URL\n",
    "for url in product_urls[:3]:\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  \n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    breadcrumbs = soup.select('div#wayfinding-breadcrumbs_feature_div ul a')\n",
    "    category = breadcrumbs[-2].get_text(strip=True) if len(breadcrumbs) > 1 else None\n",
    "    sub_category = breadcrumbs[-1].get_text(strip=True) if breadcrumbs else None\n",
    "\n",
    "    product_info = {\n",
    "        'product_url': url,\n",
    "        'Category': category,\n",
    "        'Sub-Category': sub_category\n",
    "    }\n",
    "\n",
    "    details_fields = [\n",
    "        ('title', 'span', 'id', 'productTitle'),\n",
    "        ('price', 'span', 'class_', 'a-price'),\n",
    "        ('rating', 'span', 'class_', 'a-icon-alt'),\n",
    "        ('reviews', 'span', 'id', 'acrCustomerReviewText')\n",
    "    ]\n",
    "\n",
    "    for field in details_fields:\n",
    "        try:\n",
    "            element = soup.find(field[1], {field[2]: field[3]})\n",
    "            product_info[field[0]] = element.text.strip() if element else 'N/A'\n",
    "        except AttributeError:\n",
    "            product_info[field[0]] = 'N/A'\n",
    "\n",
    "    product_description_div = soup.find('div', id='productDescription')\n",
    "    product_info['product-description'] = product_description_div.get_text(strip=True) if product_description_div else 'N/A'\n",
    "\n",
    "    write_to_file(product_info)  # Write initial product info to file\n",
    "\n",
    "    # Extract colors, images, and reviews\n",
    "    product_info['Colors'] = extract_colors(driver, product_info)\n",
    "    product_info['Reviews'] = extract_reviews(driver, product_info)\n",
    "\n",
    "    # Print the scraped data\n",
    "    print(f\"Scraped: {product_info['title']} - Category: {category}, Sub-Category: {sub_category}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
